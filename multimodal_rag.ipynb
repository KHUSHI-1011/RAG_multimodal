{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMX6a5GQRxySdEVkpqfe0ez"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-6JxsmI1eD2O"},"outputs":[],"source":["pip install \"unstructured[all-docs]\" pillow pydantic lxml matplotlib"]},{"cell_type":"code","source":["!sudo apt-get update"],"metadata":{"id":"V0CHQZM5egJq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt-get install poppler-utils"],"metadata":{"id":"q4SJ2tEPehKj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn"],"metadata":{"id":"Y_oGXKzseplb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install unstructured-pytesseract\n","!pip install tesseract-ocr"],"metadata":{"id":"8hhqjpNSeqR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from unstructured.partition.pdf import partition_pdf"],"metadata":{"id":"B4jw0TQdetqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"xQQj0_U2evpW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_pdf_elements=partition_pdf(\n","    filename=\"file from your system\",                   #mandatory,  upload your file\n","    strategy=\"hi_res\",                                  #mandatory to use \"hi_res\" strategy\n","    extract_images_in_pdf=True,                         #mandatory to use set as True\n","    extract_image_block_types=[\"Image\", \"Table\"],       #optional\n","    extract_image_block_to_payload=False,               #optional\n","    extract_image_block_output_dir=\"extracted_data\"\n","\n",")"],"metadata":{"id":"owRQZMreeyFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_pdf_elements"],"metadata":{"id":"2L_olsi4e3DT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Text\n","Header=[]\n","Footer=[]\n","Title=[]\n","NarrativeText=[]\n","Text=[]\n","ListItem=[]\n","for element in raw_pdf_elements:\n","  if \"unstructured.documents.elements.Header\" in str(type(element)):\n","    Header.append(str(element))\n","  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n","    Footer.append(str(element))\n","  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n","    Title.append(str(element))\n","  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n","    NarrativeText.append(str(element))\n","  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n","    Text.append(str(element))\n","  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n","   ListItem.append(str(element))"],"metadata":{"id":"1H--FjfUe3sP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NarrativeText"],"metadata":{"id":"-dHQqrcwe6A5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img=[]\n","for element in raw_pdf_elements:\n","  if \"unstructured.documents.elements.Image\" in str(type(element)):\n","    img.append(str(element))"],"metadata":{"id":"25ZIxet5e-tO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img"],"metadata":{"id":"gdl0zICNfCWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img[0]"],"metadata":{"id":"FcUVYO9hfESc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img[6-8]"],"metadata":{"id":"i_-NDVV_fGou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_pdf_elements2=partition_pdf(\n","    filename=\"file.pdf\",                                #mandatory,  upload your file\n","    strategy=\"hi_res\",                                  #mandatory to use \"hi_res\" strategy\n","    extract_images_in_pdf=True,                         #mandatory to use set as True\n","    extract_image_block_types=[\"Image\", \"Table\"],       #optional\n","    extract_image_block_to_payload=False,               #optional\n","    extract_image_block_output_dir=\"extracted_data\"\n","\n",")"],"metadata":{"id":"ShP_hungfIfq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_pdf_elements2"],"metadata":{"id":"u7bHaDTdfKXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img=[]\n","for element in raw_pdf_elements2:\n","  if \"unstructured.documents.elements.Image\" in str(type(element)):\n","    img.append(str(element))"],"metadata":{"id":"jZev0tNGfQt7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img"],"metadata":{"id":"xRXq2VurfSrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tab=[]\n","for element in raw_pdf_elements2:\n","  if \"unstructured.documents.elements.Table\" in str(type(element)):\n","    tab.append(str(element))"],"metadata":{"id":"oDuIiGCVfTG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tab"],"metadata":{"id":"CjArwwozfVyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tab[0]"],"metadata":{"id":"w5GzsGo9fX61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NarrativeText=[]\n","for element in raw_pdf_elements2:\n","  if \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n","    NarrativeText.append(str(element))"],"metadata":{"id":"eTWYF_oAfaKt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NarrativeText"],"metadata":{"id":"TDtCYHiJfcJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ListItem=[]\n","for element in raw_pdf_elements2:\n","  if \"unstructured.documents.elements.ListItem\" in str(type(element)):\n","    ListItem.append(str(element))"],"metadata":{"id":"EmkWac05fgqb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ListItem"],"metadata":{"id":"5ArIYAw4fi8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain_core"],"metadata":{"id":"dKkTFy8Mfliy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install google-generativeai==0.8.5 google-ai-generativelanguage==0.6.15 langchain-google-genai\n","!pip install langchain\n","!pip install chromadb\n","!pip install langchain_community\n"],"metadata":{"id":"4ndZ_Q4sfpMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(tab)"],"metadata":{"id":"veft0tPJfrmW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(img)"],"metadata":{"id":"lvAv-mfoft1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser  # same as before\n","from langchain_core.prompts import ChatPromptTemplate      # same as before\n","from langchain_google_genai import ChatGoogleGenerativeAI  # Gemini instead of OpenAI\n"],"metadata":{"id":"Ua8Q3xI7fvpL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Table Summary"],"metadata":{"id":"-_uJO6OQ4LQ5"}},{"cell_type":"code","source":["#prompt\n","prompt_text=\"\"\"You are an assistant tasked with summarizing tables for retrieval.\\\n","These summaries will be embedded and used to retrieve the raw table elements.\\\n","Give a concise summary of the table that is well optimized for retrieval.\\ Table {element}\"\"\""],"metadata":{"id":"8kbkoEUnfx1L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt= ChatPromptTemplate.from_template(prompt_text)"],"metadata":{"id":"PhLJ4YLYfz1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","# Get your Gemini API key securely\n","os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"],"metadata":{"id":"lAWmjgXqf2Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","# Text summary chain with Gemini\n","model = ChatGoogleGenerativeAI(\n","    model=\"gemini-1.5-flash\",  # or \"gemini-1.5-pro\" for better reasoning\n","    temperature=0,\n","    google_api_key=os.environ[\"GOOGLE_API_KEY\"]\n",")"],"metadata":{"id":"P5o5E38If31a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summarize_chain={\"element\":lambda x: x} | prompt | model | StrOutputParser()"],"metadata":{"id":"pwvpf-AYf3_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["table_summaries=[]"],"metadata":{"id":"kzIJFg2kf86O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["table_summaries=summarize_chain.batch(tab,{\"max_concurrency\": 5})"],"metadata":{"id":"K12kNev9gAvE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["table_summaries[0]"],"metadata":{"id":"ZgCL1iqQgBfl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Text Summary"],"metadata":{"id":"6PHWk9_c4U00"}},{"cell_type":"code","source":["#prompt\n","prompt_text=\"\"\"You are an assistant tasked with summarizing text for retrieval.\\\n","These summaries will be embedded and used to retrieve the raw text elements.\\\n","Give a concise summary of the table or text that is well optimized for retrieval.text: {element}\"\"\""],"metadata":{"id":"GCg_JhKwgFAV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt= ChatPromptTemplate.from_template(prompt_text)"],"metadata":{"id":"axqSgf5qgHOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# ✅ Use a valid model name:\n","model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"element\"],\n","    template=\"{element}\"\n",")\n","\n","summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n"],"metadata":{"id":"wzXCx4tGgJL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import random\n","\n","def safe_batch_summarize(texts, chain, max_requests_per_minute=15, max_retries=3):\n","    results = []\n","    delay = 60.0 / max_requests_per_minute\n","\n","    for i, t in enumerate(texts):\n","        for attempt in range(max_retries):\n","            try:\n","                result = chain.invoke({\"element\": t})\n","                results.append(result)\n","                break\n","            except Exception as e:\n","                print(f\"[{i}] Attempt {attempt+1}/{max_retries} failed: {e}\")\n","                if attempt < max_retries - 1:\n","                    wait = delay * (attempt + 1) + random.uniform(1, 3)\n","                    print(f\"Retrying in {wait:.1f}s...\")\n","                    time.sleep(wait)\n","                else:\n","                    results.append(f\"[ERROR] Failed to summarize: {e}\")\n","        time.sleep(delay + random.uniform(0, 1))\n","\n","    return results\n"],"metadata":{"id":"2SYA15WkgLMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ✅ Use safe batching\n","text_summaries = safe_batch_summarize(Text, summarize_chain)\n","text_summaries\n"],"metadata":{"id":"gLmQU-bZgOqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_summaries"],"metadata":{"id":"jHvzpen0gRUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import base64\n","import os\n","from langchain_core.messages import HumanMessage"],"metadata":{"id":"p_rGEHitgUEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_image(image_path):\n","  \"\"\"Getting the base64 string\"\"\"\n","  with open(image_path, \"rb\") as image_file:\n","    return base64.b64encode(image_file.read()).decode(\"utf-8\")"],"metadata":{"id":"bQnMVyR7gV_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","def image_summarize(img_base64, prompt):\n","    \"\"\"Summarize an image using Gemini\"\"\"\n","    chat = ChatGoogleGenerativeAI(\n","        model=\"gemini-1.5-flash\",  # or \"gemini-1.5-pro\" for better quality\n","        max_output_tokens=1024\n","    )\n","\n","    msg = chat.invoke(\n","        [\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": prompt},\n","                    {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{img_base64}\"}\n","                ]\n","            }\n","        ]\n","    )\n","\n","    return msg.content  # returns the text output\n"],"metadata":{"id":"Vah8OnEggXuN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import base64\n","\n","def encode_image(image_path):\n","    \"\"\"Getting the base64 string from a file\"\"\"\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n","\n","def generate_img_summaries(path):\n","    \"\"\"\n","    Generate summaries and base64 encoded strings for images.\n","    path: folder containing .jpg files OR a single image path OR a list of image paths\n","    \"\"\"\n","    img_base64_list = []\n","    image_summaries = []\n","\n","    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval.\n","These summaries will be embedded and used to retrieve the raw image.\n","Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n","\n","    # Determine what type of input path is\n","    if os.path.isdir(path):\n","        image_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".jpg\")]\n","    elif isinstance(path, list):\n","        image_files = path\n","    else:  # single image file\n","        image_files = [path]\n","\n","    for img_path in image_files:\n","        base64_image = encode_image(img_path)\n","        img_base64_list.append(base64_image)\n","        image_summaries.append(image_summarize(base64_image, prompt))\n","\n","    return img_base64_list, image_summaries\n"],"metadata":{"id":"pUylS9FggXxp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fpath=\"/content/extracted_data/\""],"metadata":{"id":"GO7-OfBUgeKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fpath = \"/content/extracted_data/figure-11-13.jpg\"  # single image file\n","img_base64_list, image_summaries = generate_img_summaries(fpath)\n","\n","print(\"Summary:\", image_summaries[0])\n"],"metadata":{"id":"oxKqQhBCghO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_summaries"],"metadata":{"id":"Xxj0qsLvgkDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_base64_list"],"metadata":{"id":"Etf1U66TgmDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import uuid\n","from langchain.retrievers.multi_vector import MultiVectorRetriever\n","from langchain.storage import InMemoryStore\n","from langchain_community.vectorstores import Chroma\n","from langchain_core.documents import Document\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"],"metadata":{"id":"Y6d9cQXMgoDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","\n","def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n","    \"\"\"\n","    Create retriever that indexes summaries, but returns raw images or texts\n","    \"\"\"\n","    # Initialize the storage layer\n","    store = InMemoryStore()\n","    id_key = \"doc_id\"\n","\n","    # Create the multi-vector retriever\n","    retriever = MultiVectorRetriever(\n","        vectorstore=vectorstore,\n","        docstore=store,\n","        id_key=id_key,\n","    )\n","\n","    # Helper function to add documents to the vectorstore and docstore\n","    def add_documents(retriever, doc_summaries, doc_contents):\n","        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n","        summary_docs = [\n","            Document(page_content=s, metadata={id_key: doc_ids[i]})\n","            for i, s in enumerate(doc_summaries)\n","        ]\n","        retriever.vectorstore.add_documents(summary_docs)\n","        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n","\n","    # Add texts, tables, and images\n","    if text_summaries:\n","        add_documents(retriever, text_summaries, texts)\n","    if table_summaries:\n","        add_documents(retriever, table_summaries, tables)\n","    if image_summaries:\n","        add_documents(retriever, image_summaries, images)\n","\n","    return retriever\n","\n","\n","# Use Gemini embeddings instead of OpenAI embeddings\n","vectorstore = Chroma(\n","    collection_name=\"mm_rag\",\n","    embedding_function=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",")\n","\n","# Create retriever with correct variable names\n","retriever_multi_vector_img = create_multi_vector_retriever(\n","    vectorstore,\n","    text_summaries,\n","    Text,              # Text variable you used earlier\n","    table_summaries,\n","    tab,               # ✅ FIXED: use tab (your actual table data variable)\n","    image_summaries,\n","    img_base64_list\n",")\n"],"metadata":{"id":"_SbKbK1Zgqhl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever_multi_vector_img"],"metadata":{"id":"hNSgq6_ngtEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import io\n","import re\n","from Ipython.display import HTML, display\n","from PIL import Image"],"metadata":{"id":"u6V8CuHRgwJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plt_img_base64(img_base64):\n","  \"\"\" Display base64 encoded string as image\"\"\"\n","  #create an html img tag with the base64 string as source\n","  image_html=f'<img src=\"data:image/jpeg;base64,{img_base64}\"/>'\n","  #Display the image by rendering with HTML\n","  display(HTML(image_html))"],"metadata":{"id":"8XR_HV0ugzQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt_img_base64(img_base64_list[1])"],"metadata":{"id":"cw4o5x52g14d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_summaries[1]"],"metadata":{"id":"iERTuBe-g6fH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def looks_like_base64(sb):\n","  \"\"\"Check if the string looks like base64\"\"\"\n","  return re.match(\"^[A-Za-z0-9+/]+[=](0,2)$\", sb) is not None"],"metadata":{"id":"rX1VHnoOg7UQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_image_data(b64data):\n","\"\"\"\n","Check if the base64 data is an image by looking at the start of the data\n","\"\"\"\n","image_signatures = {\n","\n","  b\"\\xFF\\xDB\\xFF\": \"jpg\",\n","  b\"\\x89\\x50\\x4E\\x47\\x80\\x84\\x14\\x0A\": \"png\",\n","  b\"\\x47x49\\x46\\x38\": \"gif\",\n","  b\"\\x52\\x49\\x46\\x46\": \"webp\",\n","}\n","\n","try:\n","  header =base64.b64decode(b64data) [:8] #Decode and get the first & bytes\n","  for sig, format in image_signatures.items():\n","    if header.startswith(sig):\n","      return True\n","  return False\n","except Exception:\n","  return False"],"metadata":{"id":"o4auR1eZg9yC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def resize_base64_image(base64_string, size=(128, 128)):\n","\"\"\"\n","Resize an image encoded as a Base64 string\n","\"\"\"\n","#Decode the Base64 string\n","\n","img_data =base64.b64decode(base64_string)\n","img = Image.open(io. BytesIO(img_data))\n","\n","#Resize the image\n","resized_img =img.resize(size, Image. LANCZOS)\n","\n","#Save the resized image to a bytes buffer\n","buffered =io.BytesIO()\n","resized_ing.save(buffered, format=img.format)\n","\n","#Encode the resized image to Base64\n","return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"],"metadata":{"id":"zR8hix7LhANp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_image_text_types(docs):\n","  \"\"\"\n","  Split base64-encoded images and texts\n","  \"\"\"\n","  b64_images = []\n","  texts = []\n","  for doc in docs:\n","    #Check if the document is of type Document and extract page_content if so\n","    if isinstance(doc, Document):\n","      doc=doc.page_content\n","    if looks_like_base64(doc) and is_image_data(doc):\n","      doc=resize_base64_image(doc, size=(1300, 600))\n","      b64_images.append(doc)\n","    else:\n","      texts.append(doc)\n","  print(b64_images)\n","  print(texts)\n","\n","  return {\"images\": b64_images, \"texts\": texts}"],"metadata":{"id":"qdL6BPiUhDQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def img_prompt_func(data_dict):\n","  \"\"\"Join the context into a single string\"\"\"\n","  print(data_dict)\n","  formatted_texts = \"\\n\".join(data_dict[\"context\"] [\"texts\"])\n","  messages = []\n","  #Adding image(s) to the messages if present\n","  if data_dict[\"context\"][\"images\"]:\n","    for image in data_dict[\"context\"][\"images\"]:\n","      image_message = {\n","          \"type\": \"image_url\",\n","          \"image_url\": {\"url\": f\"data:image/jpeg;base64, {image}\"}\n","      }\n","      messages.append(image_message)\n","  #Adding the text for analysis\n","  text_message={\n","      \"type\": \"text\",\n","      \"text\": (\n","           \"You are a helpful assistant.\\n\"\n","           \"You will be given a mixed info(s) .\\n\"\n","           \"Use this information to provide relevant information to the user question.\\n\"\n","           f\"User-provided question: {data_dict['question']}\\n\\n\"\n","           \"Text and / or tables: \\n\"\n","           f\"(formatted_texts)\"\n","      ),\n","\n","  }\n","messages.append(text_message)\n","return [HumanMessage(content=messages)]"],"metadata":{"id":"pRegrqythGFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda , RunnablePassthrough"],"metadata":{"id":"Es10QgKXhI-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","def multi_modal_rag_chain(retriever):            #main method\n","    \"\"\"Multi-modal RAG chain\"\"\"\n","    # Multi-modal LLM\n","    model = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-1.5-flash\", max_tokens=1024)\n","\n","    # RAG pipeline\n","    chain = (\n","        {\n","            \"context\": retriever | RunnableLambda(split_image_text_types),\n","            \"question\": RunnablePassthrough(),\n","        }\n","        | RunnableLambda(img_prompt_func)\n","        | model\n","        | StrOutputParser()\n","    )\n","    return chain\n"],"metadata":{"id":"PAnn__9fhL4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create RAG chain\n","chain_multimodal_rag= multi_modal_rag_chain(retriever_multi_vector_img)"],"metadata":{"id":"E_CHkTX_hNeU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain_multidal_rag"],"metadata":{"id":"MFTtUfGjhQoW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check"],"metadata":{"id":"Z9I70EsB4jI1"}},{"cell_type":"markdown","source":[],"metadata":{"id":"jAEiiRfS4jFY"}},{"cell_type":"code","source":["#check retrieval\n","query =\"Why We combine a pre-trained retriever (Query Encoder Document Index) with a pre-trained seq2seq model (Generator) and fine-tune\"\n","docs= retriever_multi_vector_ing.invoke(query)"],"metadata":{"id":"aavYCyvghSWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs"],"metadata":{"id":"voLKcWTkhU4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query=\"Open-Domain QA Test Scores. Scores. For TQA,\\\n","left column uses the standard test set for Open-\\\n","Domain QA, right column uses the TQA-Wiki\\\n","test set. See Appendix D for further details.\""],"metadata":{"id":"J93B8hhZhXuT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" docs =retriever_multi_vector_img.invoke(query)"],"metadata":{"id":"-kJ_65IAhZjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs"],"metadata":{"id":"BRW3rb2WhbZW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query=\"Models are trained with either 5 or 10 retrieved latent\\\n","documents, and we do not observe significant differences in performance between them.\""],"metadata":{"id":"5mh-5-NChdnr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever_multi_vector_img.invoke(query)"],"metadata":{"id":"WE3mmRyphfYo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#We get back relevant images\n","plt_img_base64(docs)"],"metadata":{"id":"D2B5zc42hhYw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Query"],"metadata":{"id":"9Kz2y50f4pi-"}},{"cell_type":"code","source":["query=\"can you explain me this Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n","In NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\"\n"],"metadata":{"id":"6MLXbzC-hlR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query1=\"Explain any images/figures in the paper with Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n"," in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\""],"metadata":{"id":"O-yJ6huhhmGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run RAG chain\n","chain_multimodal_rag.invoke(query)"],"metadata":{"id":"mHEyOY33hoN1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run RAG chain\n","chain_multimodal_rag.invoke(query1)"],"metadata":{"id":"T8zVH8rdhqgt"},"execution_count":null,"outputs":[]}]}